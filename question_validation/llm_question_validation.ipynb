{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Union\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from llm_assessor import (\n",
    "    ranged_error\n",
    ")\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def extract_total_cost(x: Union[int, Dict]) -> float:\n",
    "\n",
    "    try: \n",
    "        z = literal_eval(x)[\"total_cost\"]\n",
    "    except ValueError: \n",
    "        z = x\n",
    "    except TypeError:\n",
    "        z = x\n",
    "    \n",
    "    return float(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory - Not required if using Jupyter outside of VScode\n",
    "workdir = os.environ[\"workdir\"]\n",
    "os.chdir(workdir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataframe\n",
    "data_dir = Path(\"./validation_results\")\n",
    "\n",
    "teacher_marked_questions = pd.read_csv(data_dir / \"processed_data\" / \"student_answers_llm_graded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_types_to_exclude = []\n",
    "teacher_marked_questions_reduced = teacher_marked_questions[~teacher_marked_questions.question_type.isin(question_types_to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Costing\n",
    "teacher_marked_questions_reduced['llm_grading_total_cost'] = teacher_marked_questions_reduced.apply(lambda x: extract_total_cost(x.llm_graded_answer_token_costing) + extract_total_cost(x.llm_awarded_marks_token_costing), axis=1)\n",
    "\n",
    "token_cost_by_question_type = teacher_marked_questions_reduced.groupby(by=[\"subject_id\", \"question_type\"], as_index=False).agg({\"llm_grading_total_cost\": [\"mean\", \"sum\"], \"question_id\": \"count\"})\n",
    "token_cost_by_question_type.columns = ['subject_id', 'question_type', 'mean_llm_grading_total_cost', 'sum_llm_grading_total_cost', 'number_of_questions']\n",
    "\n",
    "token_cost_by_question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitrate\n",
    "## Mean overall\n",
    "mean_hitrate_by_subject = teacher_marked_questions_reduced.groupby(by=[\"subject_id\"], as_index = False).agg({\"question_id\": \"count\", \"llm_mark_hitrate\": [\"sum\", np.nanmean]}).rename(columns={\"question_id\": \"number_of_questions\", \"llm_mark_hitrate\": \"proportion_of_questions_covered\"})\n",
    "mean_hitrate_by_subject.columns = [\"_\".join(col_name).rstrip('_') for col_name in mean_hitrate_by_subject.columns]\n",
    "mean_hitrate_by_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Level hitrate\n",
    "## Mean overall\n",
    "mean_same_level_hitrate_by_subject = teacher_marked_questions_reduced.groupby(by=[\"subject_id\"], as_index = False).agg({\"question_id\": \"count\", \"llm_level_hitrate\": [\"sum\", np.nanmean]}).rename(columns={\"question_id\": \"number_of_questions\", \"llm_level_hitrate\": \"proportion_of_questions_covered\"})\n",
    "mean_same_level_hitrate_by_subject.columns = [\"_\".join(col_name).rstrip('_') for col_name in mean_same_level_hitrate_by_subject.columns]\n",
    "mean_same_level_hitrate_by_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same level pm1 hitrate\n",
    "## Mean overall\n",
    "# mean_same_level_pm1_hitrate_by_subject = teacher_marked_questions_reduced.groupby(by=[\"subject_id\"], as_index = False).agg({\"question_id\": \"count\", \"same_level_hitrate_pm1\": [\"sum\", np.nanmean]}).rename(columns={\"question_id\": \"number_of_questions\", \"same_level_hitrate_pm1\": \"proportion_of_questions_covered\"})\n",
    "# mean_same_level_pm1_hitrate_by_subject.columns = [\"_\".join(col_name).rstrip('_') for col_name in mean_same_level_pm1_hitrate_by_subject.columns]\n",
    "# mean_same_level_pm1_hitrate_by_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitrate\n",
    "## Mean by question type\n",
    "mean_hitrate_by_question_type = teacher_marked_questions_reduced.groupby(by=[\"subject_id\", \"question_type\"], as_index = False).agg({\"question_id\": \"count\", \"llm_mark_hitrate\": [\"sum\", np.nanmean]}).rename(columns={\"question_id\": \"number_of_questions\", \"llm_mark_hitrate\": \"mark_hitrate_proportion_of_questions_covered\"})\n",
    "mean_hitrate_by_question_type.columns = [\"_\".join(col_name).rstrip('_') for col_name in mean_hitrate_by_question_type.columns]\n",
    "\n",
    "mean_hitrate_by_question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level Hitrate\n",
    "## Mean by question type\n",
    "mean_level_hitrate_by_question_type = teacher_marked_questions_reduced.groupby(by=[\"subject_id\", \"question_type\"], as_index = False).agg({\"question_id\": \"count\", \"llm_level_hitrate\": [\"sum\", np.nanmean]}).rename(columns={\"question_id\": \"number_of_questions\", \"llm_level_hitrate\": \"level_hitrate_proportion_of_questions_covered\"})\n",
    "mean_level_hitrate_by_question_type.columns = [\"_\".join(col_name).rstrip('_') for col_name in mean_level_hitrate_by_question_type.columns]\n",
    "\n",
    "mean_level_hitrate_by_question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitrate by Question ID\n",
    "question_hit_rate = teacher_marked_questions_reduced.groupby(by=['subject_id','question_id', \"question_type\", \"total_marks\"], as_index=False).agg({\"llm_mark_hitrate\": [\"count\", \"sum\", np.nanmean]})\n",
    "question_hit_rate.columns = [\"subject_id\",\"question_id\", \"question_type\", \"total_marks\", \"number_of_questions\", \"hitrate_frequency\", \"mean_hitrate\"]\n",
    "\n",
    "question_hit_rate = question_hit_rate.sort_values(by=[\"subject_id\",\"question_id\",\"question_type\", \"mean_hitrate\"], ascending=[True, True, True, True])\n",
    "question_hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level Hitrate by Question ID\n",
    "level_question_hit_rate = teacher_marked_questions_reduced.groupby(by=['subject_id','question_id', \"question_type\", \"total_marks\"], as_index=False).agg({\"llm_level_hitrate\": [\"count\", \"sum\", np.nanmean]})\n",
    "level_question_hit_rate.columns = [\"subject_id\",\"question_id\", \"question_type\", \"total_marks\", \"number_of_questions\", \"level_hitrate_frequency\", \"mean_level_hitrate\"]\n",
    "\n",
    "level_question_hit_rate = level_question_hit_rate.sort_values(by=[\"subject_id\",\"question_id\",\"question_type\", \"mean_level_hitrate\"], ascending=[True, True, True, True])\n",
    "level_question_hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error\n",
    "teacher_marked_questions_reduced[\"marks_mean_absolute_error\"] = teacher_marked_questions_reduced.apply(lambda row: ranged_error(x=row['llm_awarded_marks'], range_of_values=[row['awarded_marks']]), axis=1)\n",
    "\n",
    "# Overall\n",
    "marks_mea = np.mean(teacher_marked_questions_reduced.marks_mean_absolute_error)\n",
    "\n",
    "## Mean by question type\n",
    "marks_mea_by_question_type = teacher_marked_questions_reduced.groupby(by=[\"subject_id\", \"question_type\"], as_index = False).agg({\"question_id\": \"count\", \"marks_mean_absolute_error\": \"mean\"}).rename(columns={\"question_id\": \"number_of_questions\"})\n",
    "marks_mea_by_question_type = marks_mea_by_question_type.sort_values(by=[\"subject_id\", \"question_type\", \"number_of_questions\"], ascending=[True, True, True])\n",
    "\n",
    "print(f\"Overall Mean:  {marks_mea:.2f}\")\n",
    "marks_mea_by_question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean scaled error metric\n",
    "\n",
    "# Overall\n",
    "scaled_metric_mean = np.nanmean(teacher_marked_questions_reduced.scaled_error_metric)\n",
    "\n",
    "## Mean by question type\n",
    "scaled_metric_mean_by_question_type = teacher_marked_questions_reduced.groupby(by=[\"subject_id\", \"question_type\"], as_index = False).agg({\"question_id\": \"count\", \"scaled_error_metric\": \"mean\"}).rename(columns={\"question_id\": \"number_of_questions\"})\n",
    "scaled_metric_mean_by_question_type = scaled_metric_mean_by_question_type.sort_values(by=[\"subject_id\", \"question_type\", \"number_of_questions\"], ascending=[True, True, True])\n",
    "\n",
    "print(f\"Overall Mean:  {scaled_metric_mean:.2f}\")\n",
    "scaled_metric_mean_by_question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Mean Absolute Error by question deviation\n",
    "question_type_mark_deviations = teacher_marked_questions_reduced.groupby(by=[\"question_type\", \"marks_mean_absolute_error\"], as_index = False).agg({\"question_id\": \"count\"})\n",
    "question_type_totals = teacher_marked_questions_reduced.groupby(by=[\"subject_id\", \"question_type\"], as_index = False).agg({\"question_id\": \"count\"})\n",
    "\n",
    "question_type_mark_deviation_joined = pd.merge(question_type_mark_deviations, question_type_totals, on = \"question_type\", how=\"left\")\n",
    "question_type_mark_deviation_joined['questions_proportion'] = question_type_mark_deviation_joined.apply(lambda x: x['question_id_x']/ x['question_id_y'], axis = 1)\n",
    "\n",
    "question_type_mark_deviation_joined = question_type_mark_deviation_joined.rename(columns={\"question_id_x\": \"number_of_questions\", \"question_id_y\": \"number_of_questions_by_question_type\"})\n",
    "ordered_cols = [\"subject_id\", \"question_type\", \"marks_mean_absolute_error\", \"number_of_questions\", \"number_of_questions_by_question_type\", \"questions_proportion\"]\n",
    "question_type_mark_deviation_joined = question_type_mark_deviation_joined[ordered_cols]\n",
    "question_type_mark_deviation_joined = question_type_mark_deviation_joined.sort_values(by=[\"subject_id\", \"question_type\", \"marks_mean_absolute_error\"], ascending=[True, True, True]).reset_index(drop=True)\n",
    "\n",
    "question_type_mark_deviation_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_stats = teacher_marked_questions_reduced.groupby(by=[\"subject_id\"], as_index=False).agg({\n",
    "    \"llm_mark_hitrate\": 'mean', \n",
    "    'llm_level_hitrate': 'mean', \n",
    "    #'same_level_hitrate_pm1': 'mean', \n",
    "    'scaled_error_metric': 'mean', \n",
    "    'question_id': 'count', \n",
    "    'elapsed_time_in_seconds': 'sum', \n",
    "    'llm_grading_total_cost': 'sum'})\n",
    "\n",
    "overall_stats['mean_hitrate'] = overall_stats['llm_mark_hitrate'].apply(lambda x: np.round(x, 2))\n",
    "overall_stats['mean_llm_level_hitrate'] = overall_stats['llm_level_hitrate'].apply(lambda x: np.round(x, 2))\n",
    "#overall_stats['mean_same_level_hitrate_pm1'] = overall_stats['same_level_hitrate_pm1'].apply(lambda x: np.round(x, 2))\n",
    "overall_stats['mean_scaled_error_metric'] = overall_stats['scaled_error_metric'].apply(lambda x: np.round(x, 2))\n",
    "\n",
    "\n",
    "overall_stats['number_of_questions'] = overall_stats.question_id\n",
    "overall_stats['elapsed_time_in_minutes'] = overall_stats['elapsed_time_in_seconds'].apply(lambda x: x/ 60)\n",
    "overall_stats['llm_grading_total_cost'] = overall_stats['llm_grading_total_cost'].apply(lambda x: np.round(x, 2))\n",
    "\n",
    "overall_stats = overall_stats[[\n",
    "    'subject_id', \n",
    "    'mean_hitrate', \n",
    "    'mean_llm_level_hitrate', \n",
    "    #'mean_same_level_hitrate_pm1', \n",
    "    'mean_scaled_error_metric', \n",
    "    'number_of_questions', \n",
    "    'elapsed_time_in_minutes', \n",
    "    'llm_grading_total_cost']]\n",
    "\n",
    "overall_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation statistics tables\n",
    "savedir = Path(workdir) / \"validation_results\" / \"validation_statistics\"\n",
    "\n",
    "Path(savedir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_marked_questions.to_csv(savedir / \"examiner_llm_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_stats.to_csv(savedir / \"mean_hitrate.csv\", index=False)\n",
    "mean_hitrate_by_question_type.to_csv(savedir / \"mean_hitrate_by_question_type.csv\", index=False)\n",
    "mean_level_hitrate_by_question_type.to_csv(savedir / \"mean_level_hitrate_by_question_type.csv\", index=False)\n",
    "marks_mea_by_question_type.to_csv(savedir / \"mea_marks_by_question_type.csv\", index=False)\n",
    "question_type_mark_deviation_joined.to_csv(savedir / \"question_type_marks_deviation.csv\", index=False)\n",
    "question_hit_rate.to_csv(savedir / \"question_hit_rate.csv\", index=False)\n",
    "level_question_hit_rate.to_csv(savedir / \"level_question_hit_rate.csv\", index=False)\n",
    "token_cost_by_question_type.to_csv(savedir / \"token_cost_by_question_type.csv\", index=False)\n",
    "scaled_metric_mean_by_question_type.to_csv(savedir / \"scaled_metric_mean_by_question_type.csv\", index = False)\n",
    "mean_same_level_hitrate_by_subject.to_csv(savedir / \"mean_same_level_hitrate_by_subject.csv\", index = False)\n",
    "#mean_same_level_pm1_hitrate_by_subject.to_csv(savedir / \"mean_same_level_pm1_hitrate_by_subject.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assessment_llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
