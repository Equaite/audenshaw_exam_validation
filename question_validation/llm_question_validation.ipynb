{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Union\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from llm_assessor import (\n",
    "    ranged_error\n",
    ")\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def extract_total_cost(x: Union[int, Dict]) -> float:\n",
    "\n",
    "    try: \n",
    "        z = literal_eval(x)[\"total_cost\"]\n",
    "    except ValueError: \n",
    "        z = x\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory - Not required if using Jupyter outside of VScode\n",
    "workdir = os.environ[\"workdir\"]\n",
    "os.chdir(workdir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataframe\n",
    "data_dir = Path(\"./validation_results\")\n",
    "\n",
    "teacher_marked_questions = pd.read_csv(data_dir / \"processed_data\" / \"student_answers_llm_graded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_marked_questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_marked_questions_reduced = teacher_marked_questions[~teacher_marked_questions.question_type.isin([\"bs_mcq\", \"bs_computation\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Costing\n",
    "teacher_marked_questions_reduced['llm_grading_total_cost'] = teacher_marked_questions_reduced.apply(lambda x: extract_total_cost(x.llm_graded_answer_token_costing) + extract_total_cost(x.llm_awarded_marks_token_costing), axis=1)\n",
    "\n",
    "token_cost_by_question_type = teacher_marked_questions.groupby(by=[\"subject_id\", \"question_type\"], as_index=False).agg({\"llm_grading_total_cost\": [\"mean\", \"sum\"], \"question_id\": \"count\"})\n",
    "token_cost_by_question_type.columns = ['subject_id', 'question_type', 'mean_llm_grading_total_cost', 'sum_llm_grading_total_cost', 'number_of_questions']\n",
    "\n",
    "token_cost_by_question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean:  0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/721clx9x1b153t3nltpkkpww0000gn/T/ipykernel_24252/3121140738.py:6: FutureWarning: The provided callable <function nanmean at 0x10af24220> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  mean_hitrate_by_question_type = teacher_marked_questions_reduced.groupby(by=[\"question_type\"], as_index = False).agg({\"question_id\": \"count\", \"llm_mark_hitrate\": [\"sum\", np.nanmean]}).rename(columns={\"question_id\": \"number_of_questions\", \"llm_mark_hitrate\": \"proportion_of_questions_covered\"})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_type</th>\n",
       "      <th>number_of_questions_count</th>\n",
       "      <th>proportion_of_questions_covered_sum</th>\n",
       "      <th>proportion_of_questions_covered_nanmean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hs_analyse</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hs_explain</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hs_judgement</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hs_spag</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_type  number_of_questions_count  \\\n",
       "0    hs_analyse                          4   \n",
       "1    hs_explain                          8   \n",
       "2  hs_judgement                          4   \n",
       "3       hs_spag                          4   \n",
       "\n",
       "   proportion_of_questions_covered_sum  \\\n",
       "0                                    0   \n",
       "1                                    3   \n",
       "2                                    1   \n",
       "3                                    2   \n",
       "\n",
       "   proportion_of_questions_covered_nanmean  \n",
       "0                                    0.000  \n",
       "1                                    0.375  \n",
       "2                                    0.250  \n",
       "3                                    0.500  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hitrate\n",
    "## Mean overall\n",
    "mean_hitrate = np.nanmean(teacher_marked_questions_reduced.llm_mark_hitrate)\n",
    "\n",
    "## Mean by question type\n",
    "mean_hitrate_by_question_type = teacher_marked_questions_reduced.groupby(by=[\"question_type\"], as_index = False).agg({\"question_id\": \"count\", \"llm_mark_hitrate\": [\"sum\", np.nanmean]}).rename(columns={\"question_id\": \"number_of_questions\", \"llm_mark_hitrate\": \"proportion_of_questions_covered\"})\n",
    "mean_hitrate_by_question_type.columns = [\"_\".join(col_name).rstrip('_') for col_name in mean_hitrate_by_question_type.columns]\n",
    "\n",
    "print(f\"Overall Mean:  {mean_hitrate:.2f}\")\n",
    "mean_hitrate_by_question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/721clx9x1b153t3nltpkkpww0000gn/T/ipykernel_24252/3803711188.py:4: FutureWarning: The provided callable <function nanmean at 0x10af24220> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  question_hit_rate = teacher_marked_questions_reduced.groupby(by=['question_id', \"question_type\", \"total_marks\"], as_index=False).agg({\"llm_mark_hitrate\": [\"count\", \"sum\", np.nanmean]})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_type</th>\n",
       "      <th>total_marks</th>\n",
       "      <th>number_of_questions</th>\n",
       "      <th>hitrate_frequency</th>\n",
       "      <th>mean_hitrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>hs_explain</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>hs_explain</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>hs_analyse</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.1</td>\n",
       "      <td>hs_judgement</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.2</td>\n",
       "      <td>hs_spag</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id question_type  total_marks  number_of_questions  \\\n",
       "0          1.0    hs_explain            4                    4   \n",
       "1          2.0    hs_explain           12                    4   \n",
       "2          3.0    hs_analyse            8                    4   \n",
       "3          4.1  hs_judgement           16                    4   \n",
       "4          4.2       hs_spag            4                    4   \n",
       "\n",
       "   hitrate_frequency  mean_hitrate  \n",
       "0                  1          0.25  \n",
       "1                  2          0.50  \n",
       "2                  0          0.00  \n",
       "3                  1          0.25  \n",
       "4                  2          0.50  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hitrate by Question ID\n",
    "teacher_marked_questions_reduced.head()\n",
    "\n",
    "question_hit_rate = teacher_marked_questions_reduced.groupby(by=['question_id', \"question_type\", \"total_marks\"], as_index=False).agg({\"llm_mark_hitrate\": [\"count\", \"sum\", np.nanmean]})\n",
    "question_hit_rate.columns = [\"question_id\", \"question_type\", \"total_marks\", \"number_of_questions\", \"hitrate_frequency\", \"mean_hitrate\"]\n",
    "\n",
    "question_hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error\n",
    "teacher_marked_questions_reduced[\"marks_mean_absolute_error\"] = teacher_marked_questions_reduced.apply(lambda row: ranged_error(x=row['llm_awarded_marks'], range_of_values=[row['awarded_marks']]), axis=1)\n",
    "\n",
    "# Overall\n",
    "marks_mea = np.mean(teacher_marked_questions_reduced.marks_mean_absolute_error)\n",
    "\n",
    "## Mean by question type\n",
    "marks_mea_by_question_type = teacher_marked_questions_reduced.groupby(by=[\"question_type\"], as_index = False).agg({\"question_id\": \"count\", \"marks_mean_absolute_error\": \"mean\"}).rename(columns={\"question_id\": \"number_of_questions\"})\n",
    "\n",
    "print(f\"Overall Mean:  {marks_mea:.2f}\")\n",
    "marks_mea_by_question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Mean Absolute Error by question deviation\n",
    "question_type_mark_deviations = teacher_marked_questions_reduced.groupby(by=[\"question_type\", \"marks_mean_absolute_error\"], as_index = False).agg({\"question_id\": \"count\"})\n",
    "question_type_totals = teacher_marked_questions_reduced.groupby(by=[\"question_type\"], as_index = False).agg({\"question_id\": \"count\"})\n",
    "\n",
    "question_type_mark_deviation_joined = pd.merge(question_type_mark_deviations, question_type_totals, on = \"question_type\", how=\"left\")\n",
    "question_type_mark_deviation_joined['questions_proportion'] = question_type_mark_deviation_joined.apply(lambda x: x['question_id_x']/ x['question_id_y'], axis = 1)\n",
    "\n",
    "question_type_mark_deviation_joined = question_type_mark_deviation_joined.rename(columns={\"question_id_x\": \"number_of_questions\", \"question_id_y\": \"number_of_questions_by_question_type\"})\n",
    "question_type_mark_deviation_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_stats = pd.DataFrame({\"mean_hitrate\": [np.round(mean_hitrate, 2)], \"marks_mean_abolute_error\": [np.round(marks_mea, 2)], \"number_of_questions\": [teacher_marked_questions.shape[0]], \"elapsed_time_in_minutes\": [np.round( np.sum(teacher_marked_questions.elapsed_time_in_seconds), 0) / 60], \"llm_grading_total_cost\": np.round( np.sum(token_cost_by_question_type.sum_llm_grading_total_cost), 2 )})\n",
    "overall_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation statistics tables\n",
    "savedir = Path(workdir) / \"validation_results\" / \"validation_statistics\"\n",
    "\n",
    "Path(savedir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_marked_questions.to_csv(savedir / \"examiner_llm_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_stats.to_csv(savedir / \"mean_hitrate.csv\", index=False)\n",
    "mean_hitrate_by_question_type.to_csv(savedir / \"mean_hitrate_by_question_type.csv\", index=False)\n",
    "marks_mea_by_question_type.to_csv(savedir / \"mea_marks_by_question_type.csv\", index=False)\n",
    "question_type_mark_deviation_joined.to_csv(savedir / \"question_type_marks_deviation.csv\", index=False)\n",
    "question_hit_rate.to_csv(savedir / \"question_hit_rate.csv\", index=False)\n",
    "token_cost_by_question_type.to_csv(savedir / \"token_cost_by_question_type\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assessment_llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
