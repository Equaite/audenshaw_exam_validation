{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from llm_assessor import (\n",
    "    run_prompt_chain,\n",
    "    extract_marks_from_graded_answer,\n",
    ")\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory - Not required if using Jupyter outside of VScode\n",
    "workdir = os.environ[\"workdir\"]\n",
    "os.chdir(workdir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Directories containing Prompt Templates and Student Answers\n",
    "prompt_dir = Path(\"prompt_templates\")\n",
    "\n",
    "data_dir = Path(f\"{workdir}/validation_results/processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o\", \n",
    "    temperature=0.00, \n",
    "    max_tokens=800,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Student Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Data\n",
    "completed_paper_df = pd.read_csv(data_dir / \"student_answers_augmented.csv\")\n",
    "student_answers_records = completed_paper_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all prompt templates\n",
    "prompts = {\n",
    "    \"grade_answer\": \n",
    "    {\n",
    "        \"aqa_history\": {\n",
    "            \"hs_analyse\": load_prompt(prompt_dir/ \"aqa_history\" / \"analyse_prompt\" / \"grade_answer_aqa_history_analyse_prompt.json\"),\n",
    "            \"hs_explain\": load_prompt(prompt_dir/ \"aqa_history\" / \"explain_prompt\" / \"grade_answer_aqa_history_explain_prompt.json\"),\n",
    "            \"hs_judgement\": load_prompt(prompt_dir/ \"aqa_history\" / \"judgement_prompt\" / \"grade_answer_aqa_history_judgement_prompt.json\"),\n",
    "            \"hs_spag\": load_prompt(prompt_dir/ \"aqa_history\" / \"spag_prompt\" / \"grade_answer_aqa_history_spag_prompt.json\")\n",
    "        },\n",
    "        \"edexcel_business_studies\": {\n",
    "            \"bs_analyse\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"analyse_prompt\" / \"grade_answer_edexcel_business_studies_analyse_prompt.json\"),\n",
    "            \"bs_discuss\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"discuss_prompt\" / \"grade_answer_edexcel_business_studies_discuss_prompt.json\"),\n",
    "            \"bs_evaluate\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"evaluate_prompt\" / \"grade_answer_edexcel_business_studies_evaluate_prompt.json\"),\n",
    "            \"bs_explain\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"explain_prompt\" / \"grade_answer_edexcel_business_studies_explain_prompt.json\"),\n",
    "            \"bs_identify\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"identify_prompt\" / \"grade_answer_edexcel_business_studies_identify_prompt.json\"),\n",
    "            \"bs_justify\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"justify_prompt\" / \"grade_answer_edexcel_business_studies_justify_prompt.json\"),\n",
    "            \"bs_outline\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"outline_prompt\" / \"grade_answer_edexcel_business_studies_outline_prompt.json\"),\n",
    "            \"bs_state\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"state_prompt\" / \"grade_answer_edexcel_business_studies_state_prompt.json\")\n",
    "        }\n",
    "    },\n",
    "    \"extract_marks\": load_prompt(prompt_dir / \"extract_mark_count\" / \"extract_mark_count_prompt.json\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade Answers for Each Student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass 1: Grade all Student Answers with Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass 1: Grade All Answers\n",
    "for _, student_answer in enumerate(student_answers_records):\n",
    "    \n",
    "    student_answer[\"start_time\"] = time.time()\n",
    "    prompt_template = prompts[\"grade_answer\"].get(student_answer[\"subject_id\"]).get(student_answer[\"question_type\"])\n",
    "    \n",
    "    if (student_answer[\"answer_text\"] is np.nan or len(student_answer[\"answer_text\"]) == 0):\n",
    "        \n",
    "        student_answer[\"llm_graded_answer\"] = \"No Answer is provided. Therefore 0 marks are awarded for this answer.\"   \n",
    "        student_answer[\"llm_graded_answer_token_costing\"] = 0\n",
    "        student_answer[\"llm_awarded_marks\"] = 0\n",
    "        student_answer[\"llm_awarded_marks_token_costing\"] = 0\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "\n",
    "            # Grade Answer\n",
    "            logging.info(f\"Starting Grading Answer for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "\n",
    "            input_args = {\n",
    "                \"question\": student_answer[\"question_text\"], \n",
    "                \"answer\": student_answer[\"answer_text\"], \n",
    "                \"mark_scheme\": student_answer[\"mark_scheme_text\"], \n",
    "                \"context\": student_answer[\"context\"]\n",
    "                }\n",
    "            prompt_template_input_args = dict((k, input_args[k]) for k in prompt_template.input_variables if k in input_args)\n",
    "\n",
    "            graded_answer_response = run_prompt_chain(\n",
    "                prompt_template=prompt_template.template,\n",
    "                llm=llm,\n",
    "                burn_in_runs=2,\n",
    "                **prompt_template_input_args\n",
    "                )\n",
    "            \n",
    "            student_answer[\"llm_graded_answer\"] = graded_answer_response[\"prompt_chain_response\"]\n",
    "            student_answer[\"llm_graded_answer_token_costing\"] = graded_answer_response[\"prompt_chain_token_costing\"]\n",
    "            \n",
    "            logging.info(f\"Completed Grading Answer for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "            \n",
    "            # Extract Marks\n",
    "            logging.info(f\"Starting Extracting Marks for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "            \n",
    "            extract_marks_response = run_prompt_chain(\n",
    "                prompt_template=prompts[\"extract_marks\"].template,\n",
    "                llm=llm,\n",
    "                burn_in_runs=1,\n",
    "                answer=student_answer[\"llm_graded_answer\"]\n",
    "                )\n",
    "            marks = extract_marks_response[\"prompt_chain_response\"]\n",
    "            marking_token_cost = extract_marks_response[\"prompt_chain_token_costing\"]\n",
    "\n",
    "            student_answer[\"llm_awarded_marks\"] = int(marks)\n",
    "            student_answer[\"llm_awarded_marks_token_costing\"] = marking_token_cost\n",
    "            \n",
    "            logging.info(f\"Completed Extracting Marks for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "\n",
    "        except:\n",
    "            logging.info(f\"Question type: {student_answer['question_type']} is not supported. Assigning NA and skipping to next question.\")\n",
    "\n",
    "            student_answer[\"llm_graded_answer\"] = \"NA\"   \n",
    "            student_answer[\"llm_graded_answer_token_costing\"] = 0\n",
    "            student_answer[\"llm_awarded_marks\"] = np.nan\n",
    "            student_answer[\"llm_awarded_marks_token_costing\"] = 0\n",
    "   \n",
    "    # Compute hitrate\n",
    "    min_mark = min( [student_answer.get(\"awarded_marks\")] )\n",
    "    max_mark = max( [student_answer.get(\"awarded_marks\")] )\n",
    "\n",
    "    if student_answer[\"awarded_marks\"] == 0:\n",
    "        student_answer[\"llm_mark_hitrate\"] = np.nan\n",
    "    else:\n",
    "        student_answer[\"llm_mark_hitrate\"]=min_mark<=student_answer[\"llm_awarded_marks\"]<=max_mark\n",
    "\n",
    "    student_answer[\"end_time\"] = time.time()\n",
    "    student_answer[\"elapsed_time_in_seconds\"] = student_answer[\"end_time\"] - student_answer[\"start_time\"]\n",
    "\n",
    "    # Randomly sleep for seconds to avoid API throttling. Between 1-3 seconds\n",
    "    seconds_to_sleep = random.sample([1,2,3], 1)[0]\n",
    "\n",
    "    logging.info(f\"Sleeping for {seconds_to_sleep} seconds to avoid API Throttling.\")\n",
    "    time.sleep(seconds_to_sleep)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Student Answers and save as a DataFrame\n",
    "completed_paper_df = pd.DataFrame(student_answers_records)\n",
    "ordered_cols = ['subject_id', 'question_id', 'question_type', 'student_id',\n",
    "'question_text', 'mark_scheme_text', 'context', 'answer_text', \n",
    "'llm_graded_answer', 'awarded_marks', 'llm_awarded_marks', 'total_marks',\n",
    "'llm_mark_hitrate', 'answer_id', 'linked_answer_id', 'topic_id', \n",
    "'answer_scanned_image', 'elapsed_time_in_seconds', 'llm_graded_answer_token_costing', 'llm_awarded_marks_token_costing']\n",
    "\n",
    "completed_paper_df = completed_paper_df[ ordered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save graded student answers\n",
    "completed_paper_df.to_csv(data_dir / \"student_answers_llm_graded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assessment_llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
