{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from llm_assessor import (\n",
    "    run_prompt_chain,\n",
    ")\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory - Not required if using Jupyter outside of VScode\n",
    "workdir = os.environ[\"workdir\"]\n",
    "os.chdir(workdir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def find_marking_level(\n",
    "        mark: int, \n",
    "        levels_dict: Dict\n",
    "        ) -> int:\n",
    "    \n",
    "    levels_found = {level for (level, marks) in levels_dict.items() if min(marks) <= mark <= max(marks)}\n",
    "\n",
    "    if len(levels_found) == 0:\n",
    "        raise ValueError(\"Mark not found in Marking Levels.\")\n",
    "\n",
    "    level = list( levels_found)[0]\n",
    "\n",
    "    return level\n",
    "\n",
    "def compute_metric(\n",
    "        llm_awarded_mark: int, \n",
    "        teacher_awarded_mark: int, \n",
    "        level_structure: Dict\n",
    "        ) -> float:\n",
    "    \n",
    "    awarded_level = find_marking_level(teacher_awarded_mark, level_structure )\n",
    "    llm_level = find_marking_level(llm_awarded_mark, level_structure)\n",
    "    difference_between_levels = abs(awarded_level - llm_level) + 1\n",
    "    \n",
    "    metric = difference_between_levels * abs(teacher_awarded_mark - llm_awarded_mark)\n",
    "    \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Directories containing Prompt Templates and Student Answers\n",
    "prompt_dir = Path(\"prompt_templates\")\n",
    "\n",
    "data_dir = Path(f\"{workdir}/validation_results/processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o\", \n",
    "    temperature=0.00, \n",
    "    max_tokens=800,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Student Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Data\n",
    "completed_paper_df = pd.read_csv(data_dir / \"student_answers_augmented.csv\")\n",
    "student_answers_records = completed_paper_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all prompt templates\n",
    "prompts = {\n",
    "    \"grade_answer\": \n",
    "    {\n",
    "        \"aqa_history\": {\n",
    "            \"hs_analyse\": load_prompt(prompt_dir/ \"aqa_history\" / \"analyse_prompt\" / \"grade_answer_aqa_history_analyse_prompt.json\"),\n",
    "            \"hs_explain\": load_prompt(prompt_dir/ \"aqa_history\" / \"explain_prompt\" / \"grade_answer_aqa_history_explain_prompt.json\"),\n",
    "            \"hs_judgement\": load_prompt(prompt_dir/ \"aqa_history\" / \"judgement_prompt\" / \"grade_answer_aqa_history_judgement_prompt.json\"),\n",
    "            \"hs_spag\": load_prompt(prompt_dir/ \"aqa_history\" / \"spag_prompt\" / \"grade_answer_aqa_history_spag_prompt.json\")\n",
    "        },\n",
    "        \"edexcel_business_studies\": {\n",
    "            \"bs_analyse\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"analyse_prompt\" / \"grade_answer_edexcel_business_studies_analyse_prompt.json\"),\n",
    "            \"bs_discuss\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"discuss_prompt\" / \"grade_answer_edexcel_business_studies_discuss_prompt.json\"),\n",
    "            \"bs_evaluate\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"evaluate_prompt\" / \"grade_answer_edexcel_business_studies_evaluate_prompt.json\"),\n",
    "            \"bs_explain\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"explain_prompt\" / \"grade_answer_edexcel_business_studies_explain_prompt.json\"),\n",
    "            \"bs_identify\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"identify_prompt\" / \"grade_answer_edexcel_business_studies_identify_prompt.json\"),\n",
    "            \"bs_justify\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"justify_prompt\" / \"grade_answer_edexcel_business_studies_justify_prompt.json\"),\n",
    "            \"bs_outline\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"outline_prompt\" / \"grade_answer_edexcel_business_studies_outline_prompt.json\"),\n",
    "            \"bs_state\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"state_prompt\" / \"grade_answer_edexcel_business_studies_state_prompt.json\"),\n",
    "            \"bs_mcq\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"mcq_prompt\" / \"grade_answer_edexcel_business_studies_mcq_prompt.json\"),\n",
    "            \"bs_calculate\": load_prompt(prompt_dir/ \"edexcel_business_studies\" / \"calculate_prompt\" / \"grade_answer_edexcel_business_studies_calculate_prompt.json\")\n",
    "        }\n",
    "    },\n",
    "    \"extract_marks\": load_prompt(prompt_dir / \"extract_mark_count\" / \"extract_mark_count_prompt.json\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade Answers for Each Student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass 1: Grade all Student Answers with Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_iter = 10\n",
    "resume_idx = 0\n",
    "use_checkpoint = True\n",
    "\n",
    "checkpoint_path = \"./validation_results/processed_data/grading_checkpoint/grading_checkpoint_890.json\"\n",
    "checkpoint_file = Path(checkpoint_path)\n",
    "\n",
    "if checkpoint_file.is_file():\n",
    "    \n",
    "    # Update iterable with cached entries\n",
    "    with open(checkpoint_path, 'r') as openfile:\n",
    "        checkpointed_records = json.load(openfile)\n",
    "    \n",
    "    # Checkpoint end index\n",
    "    resume_idx = int(checkpoint_path.split(\"_\")[-1].split(\".\")[0])\n",
    "\n",
    "    # Update iterable\n",
    "    student_answers_records[0:resume_idx] = checkpointed_records\n",
    "\n",
    "print(\"Resume Index:\\n\", resume_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass 1: Grade All Answers\n",
    "valid_types = list(set(completed_paper_df[completed_paper_df.questions_with_marking_level_flag == 1][\"question_type\"].tolist()))\n",
    "\n",
    "for idx in range(resume_idx, len(student_answers_records)):\n",
    "\n",
    "    student_answer = student_answers_records[idx]\n",
    "    student_answer[\"start_time\"] = time.time()\n",
    "    \n",
    "    if (student_answer[\"answer_text\"] is np.nan or len(student_answer[\"answer_text\"]) == 0):\n",
    "        \n",
    "        logging.info(f\"Starting Grading Answer for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "        \n",
    "        student_answer[\"llm_graded_answer\"] = \"No Answer is provided. Therefore 0 marks are awarded for this answer.\"   \n",
    "        student_answer[\"llm_graded_answer_token_costing\"] = 0\n",
    "        student_answer[\"llm_awarded_marks\"] = 0\n",
    "        student_answer[\"llm_awarded_marks_token_costing\"] = 0\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            # Grade Answer\n",
    "            logging.info(f\"Starting Grading Answer for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "            prompt_template = prompts[\"grade_answer\"].get(student_answer[\"subject_id\"]).get(student_answer[\"question_type\"])\n",
    "            \n",
    "            input_args = {\n",
    "                \"question\": student_answer[\"question_text\"], \n",
    "                \"answer\": student_answer[\"answer_text\"], \n",
    "                \"mark_scheme\": student_answer[\"mark_scheme_text\"], \n",
    "                \"context\": student_answer[\"context\"]\n",
    "                }\n",
    "            prompt_template_input_args = dict((k, input_args[k]) for k in prompt_template.input_variables if k in input_args)\n",
    "\n",
    "            graded_answer_response = run_prompt_chain(\n",
    "                prompt_template=prompt_template.template,\n",
    "                llm=llm,\n",
    "                burn_in_runs=1,\n",
    "                **prompt_template_input_args\n",
    "                )\n",
    "            \n",
    "            student_answer[\"llm_graded_answer\"] = graded_answer_response[\"prompt_chain_response\"]\n",
    "            student_answer[\"llm_graded_answer_token_costing\"] = graded_answer_response[\"prompt_chain_token_costing\"]\n",
    "            \n",
    "            logging.info(f\"Completed Grading Answer for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "            \n",
    "            # Extract Marks\n",
    "            logging.info(f\"Starting Extracting Marks for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "            \n",
    "            extract_marks_response = run_prompt_chain(\n",
    "                prompt_template=\"Extract the number of marks awarded to the student answer. Read the answer carefully and extract the final number of marks to be awarded to the student. Report the number of marks only. Answer: {answer}\",\n",
    "                llm=ChatOpenAI(model_name=\"gpt-4o\", temperature=0.0, max_tokens=500),\n",
    "                burn_in_runs=1,\n",
    "                answer=student_answer[\"llm_graded_answer\"]\n",
    "                )\n",
    "            marks = extract_marks_response[\"prompt_chain_response\"]\n",
    "            marking_token_cost = extract_marks_response[\"prompt_chain_token_costing\"]\n",
    "\n",
    "            extracted_marks = [int(i) for i in marks.split() if i.isdigit()] \n",
    "            student_answer[\"llm_awarded_marks\"] = extracted_marks[0] if len(extracted_marks) > 0 else np.nan \n",
    "            student_answer[\"llm_awarded_marks_token_costing\"] = marking_token_cost\n",
    "            \n",
    "            logging.info(f\"Completed Extracting Marks for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(e)\n",
    "            logging.info(f\"Question type: {student_answer['question_type']} is not supported. Assigning NA and skipping to next question.\")\n",
    "\n",
    "            student_answer[\"llm_graded_answer\"] = \"NA\"   \n",
    "            student_answer[\"llm_graded_answer_token_costing\"] = 0\n",
    "            student_answer[\"llm_awarded_marks\"] = np.nan\n",
    "            student_answer[\"llm_awarded_marks_token_costing\"] = 0\n",
    "\n",
    "    student_answer[\"end_time\"] = time.time()\n",
    "    student_answer[\"elapsed_time_in_seconds\"] = student_answer[\"end_time\"] - student_answer[\"start_time\"]\n",
    "\n",
    "    # Save Checkpoint\n",
    "    if (use_checkpoint) and (idx > 0) and (idx % checkpoint_iter == 0):\n",
    "\n",
    "        # Create a savedir\n",
    "        checkpoint_savedir = Path(\"./validation_results/processed_data/grading_checkpoint\")\n",
    "        checkpoint_savedir.mkdir(parents=True, exist_ok=True)\n",
    "       \n",
    "        # Save the list up until this index\n",
    "        checkpointed_records = student_answers_records[0:idx]\n",
    "\n",
    "        # Save checkpoint file as json\n",
    "        checkpoint_savename = f\"grading_checkpoint_{idx}.json\"\n",
    "        with open(checkpoint_savedir / checkpoint_savename, \"w\") as savefile:\n",
    "            json.dump(checkpointed_records, savefile)\n",
    "\n",
    "    # Randomly sleep for seconds to avoid API throttling. Between 1-3 seconds\n",
    "    seconds_to_sleep = random.sample([1,2,3], 1)[0]\n",
    "\n",
    "    logging.info(f\"Sleeping for {seconds_to_sleep} seconds to avoid API Throttling.\")\n",
    "    time.sleep(seconds_to_sleep)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hitrate, Level Hitrate and Scaled Error Metric\n",
    "for _, student_answer in enumerate(student_answers_records):\n",
    "\n",
    "    logging.info(f\"Computing Hitrate for Student ID: {student_answer['student_id']} and Question ID: {student_answer['question_id']}\")\n",
    "\n",
    "    # Hitrate\n",
    "    ## Minimum Marks and Maximum Marks from range of awarded marks\n",
    "    min_mark = min( [student_answer.get(\"awarded_marks\")] )\n",
    "    max_mark = max( [student_answer.get(\"awarded_marks\")] )\n",
    "\n",
    "    if (student_answer[\"awarded_marks\"] == 0 and student_answer[\"answer_text\"] is np.nan):\n",
    "        student_answer[\"llm_mark_hitrate\"] = np.nan\n",
    "        student_answer['same_level_hitrate'] = np.nan\n",
    "        student_answer['scaled_error_metric'] = np.nan\n",
    "    else:\n",
    "        student_answer[\"llm_mark_hitrate\"]=min_mark<=student_answer[\"llm_awarded_marks\"]<=max_mark\n",
    "        \n",
    "    # Level Hitrate and Scaled Error Metric\n",
    "    if student_answer['level_structure'] is not None and student_answer['question_type'] in valid_types:\n",
    "        \n",
    "        levels_dict = ast.literal_eval(student_answer['level_structure'])\n",
    "        max_error = max(\n",
    "            compute_metric(llm_awarded_mark = 0, teacher_awarded_mark = student_answer['awarded_marks'], level_structure = levels_dict), \n",
    "            compute_metric(llm_awarded_mark = max( max(lst) for lst in levels_dict.values() ), teacher_awarded_mark = student_answer['awarded_marks'], level_structure = levels_dict) \n",
    "            )\n",
    "        logging.info(f\"Maximum Error: \\n{max_error}\")\n",
    "        \n",
    "        try:\n",
    "            metric = compute_metric(\n",
    "                llm_awarded_mark = student_answer['llm_awarded_marks'], \n",
    "                teacher_awarded_mark = student_answer['awarded_marks'], \n",
    "                level_structure = levels_dict\n",
    "                )\n",
    "        except:\n",
    "            metric = max_error\n",
    "        finally:\n",
    "            scaled_error = metric/max_error\n",
    "            logging.info(f\"Scaled Error: \\n{scaled_error}\")\n",
    "\n",
    "        llm_awarded_level = find_marking_level(mark = student_answer['llm_awarded_marks'], levels_dict = levels_dict)\n",
    "        awarded_level = find_marking_level(mark = student_answer['awarded_marks'], levels_dict = levels_dict)\n",
    "        difference_in_level = abs(llm_awarded_level - awarded_level)\n",
    "\n",
    "        student_answer[\"llm_awarded_level\"] = llm_awarded_level\n",
    "        student_answer[\"awarded_level\"] = awarded_level\n",
    "        student_answer['llm_level_hitrate'] = int(difference_in_level == 0)\n",
    "        #student_answer['same_level_hitrate_pm1'] = int(difference_in_level <= 1 and abs(student_answer[\"awarded_marks\"] - student_answer[\"llm_awarded_marks\"]) <= 1)\n",
    "        student_answer['scaled_error_metric'] = scaled_error\n",
    "    else:\n",
    "        student_answer[\"llm_awarded_level\"] = np.nan\n",
    "        student_answer[\"awarded_level\"] = np.nan\n",
    "        student_answer['llm_level_hitrate'] = np.nan\n",
    "        #student_answer['same_level_hitrate_pm1'] = np.nan\n",
    "        student_answer['scaled_error_metric'] = np.nan            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Student Answers and save as a DataFrame\n",
    "completed_paper_df = pd.DataFrame(student_answers_records)\n",
    "ordered_cols = [\n",
    "    'subject_id', 'question_id', 'question_type', 'student_id','question_text', 'mark_scheme_text', \n",
    "    'context', 'answer_text', 'llm_graded_answer', 'awarded_marks', 'llm_awarded_marks', 'total_marks',\n",
    "    'awarded_level', 'llm_awarded_level', 'llm_mark_hitrate' , 'llm_level_hitrate', 'scaled_error_metric', 'answer_id', \n",
    "    'linked_answer_id', 'topic_id', 'answer_scanned_image', 'elapsed_time_in_seconds', 'llm_graded_answer_token_costing', 'llm_awarded_marks_token_costing'\n",
    "    ]\n",
    "\n",
    "completed_paper_df = completed_paper_df[ ordered_cols ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Mark Scheme:\\n\", student_answer[\"mark_scheme_text\"])\n",
    "# print(\"-\"*40)\n",
    "# print(\"Student Answer:\\n\", student_answer[\"answer_text\"])\n",
    "# print(\"-\"*40)\n",
    "# print(\"Graded Answer:\\n\", student_answer[\"llm_graded_answer\"])\n",
    "# print(\"-\"*40)\n",
    "# print(\"LLM Marks:\\n\", student_answer[\"llm_awarded_marks\"])\n",
    "# print(\"-\"*40)\n",
    "# print(\"Actual Marks:\\n\", student_answer[\"awarded_marks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save graded student answers\n",
    "completed_paper_df.to_csv(data_dir / \"student_answers_llm_graded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assessment_llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
